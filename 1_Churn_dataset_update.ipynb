{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf2f8f6-750f-4f11-8a55-80e0f256a390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Ldap password:\n",
      " ········\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "import getpass\n",
    "username='EZAGAYNAYA'\n",
    "REALM='BEE.VIMPELCOM.RU'\n",
    "password = getpass.getpass(prompt='Enter Your Ldap password:\\n')\n",
    "kinit = 'kinit'\n",
    "kinit_args = [ kinit, '%s@%s' % (username, REALM) ]\n",
    "kinit = Popen(kinit_args, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "kinit.stdin.write(bytearray(password + \"\\n\", \"ascii\"))\n",
    "kinit.stdin.flush()\n",
    "kinit.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a5ffa9-3e13-4e74-8d0d-761363891ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/31 11:47:31 WARN HiveConf: HiveConf of name hive.mapred.supports.subdirectories does not exist\n",
      "25/10/31 11:47:39 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf  # noqa: E402\n",
    "from pyspark.sql import DataFrame as SparkDataFrame  # noqa: E402\n",
    "from pyspark.sql import SparkSession  # noqa: E402\n",
    "from pyspark.sql import functions as F  # noqa: E402\n",
    "from pyspark.sql.functions import col, when, min, max, sum, mean, count, substring, countDistinct, last_day, concat_ws \n",
    "from pyspark.sql.types import StringType, LongType\n",
    "\n",
    "SPARK_SETTINGS = [\n",
    "    ('spark.yarn.queue', 'default'),\n",
    "        ('spark.sql.sources.partitionOverwriteMode', 'dynamic'),\n",
    "        ('hive.exec.dynamic.partition', 'true'),\n",
    "        ('hive.exec.dynamic.partition.mode', 'nonstrict'),\n",
    "        ('spark.default.parallelism', '400'),\n",
    "        ('spark.driver.memory', '20G'),\n",
    "        ('spark.driver.memoryOverhead', '2G'),\n",
    "        ('spark.driver.cores', 4),\n",
    "        ('spark.driver.maxResultSize', '150G'),\n",
    "        ('spark.dynamicAllocation.enabled', 'false'),\n",
    "        ('spark.dynamicAllocation.maxExecutors', 200),\n",
    "        ('spark.executor.memory', '20G'),\n",
    "        ('spark.executor.memoryOverhead', '4G'),\n",
    "        ('spark.executor.cores', 6),\n",
    "        ('spark.executor.instances', 10),\n",
    "        ('spark.hadoop.mapreduce.input.fileinputformat.input.dir.recursive', 'true'),\n",
    "        ('spark.hive.mapred.supports.subdirectories', 'true'),# драйвер (понадобится для чтения из RDBMS)\n",
    "        ('spark.kryoserializer.buffer', '1000m'),\n",
    "        ('spark.kryoserializer.buffer.max', '2000m'),\n",
    "        ('spark.rpc.message.maxSize', 2000),\n",
    "        ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
    "        ('spark.sql.execution.arrow.pyspark.enabled', 'true'),\n",
    "        ('spark.sql.shuffle.partitions', '400'),\n",
    "        ('spark.driver.extraClassPath', '/lib/oracle/19.8/client64/lib/ojdbc8.jar'), # драйвер (понадобится для чтения из RDBMS)\n",
    "        ('spark.jars', '/lib/oracle/19.8/client64/lib/ojdbc8.jar'),\n",
    "        (\"spark.sql.hive.convertMetastoreOrc\", \" false\"),\n",
    "        ('spark.app.name', 'SVFomenkov_testing_pipeline')\n",
    "]\n",
    "\n",
    "app_name = 'ZEM_spark_session'\n",
    "\n",
    "conf = SparkConf().setAppName(app_name).setMaster('yarn').setAll(SPARK_SETTINGS)\n",
    "spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark.conf.set(\"spark.sql.hive.convertMetastoreOrc\", \" false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e122f6-2a62-4822-aebb-e3b020c271f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5658/3383840084.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>div.output_area pre {white-space: pre;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>div.jp-OutputArea-output pre {white-space: pre;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_area pre {white-space: pre;}</style>\"))\n",
    "display(HTML(\"<style>div.jp-OutputArea-output pre {white-space: pre;}</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b0263-7cda-4a0b-893c-ab541e0d309a",
   "metadata": {},
   "source": [
    "#### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a18f521-0eea-4a6f-b714-ac0d50e9f856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/pylab.py:162: UserWarning: pylab import has clobbered these variables: ['min', 'max', 'datetime', 'mean', 'sum']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import pickle\n",
    "# import shap\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection, datasets, linear_model, metrics\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import math\n",
    "\n",
    "# TF-IDF:\n",
    "import sklearn.feature_extraction.text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9df9a2-ceef-47dd-b584-4d49c8ed78a3",
   "metadata": {},
   "source": [
    "#### df Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b761e66-1255-4dfb-8845-a7584a452637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>(398 + 2) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 93498497\n",
      "root\n",
      " |-- transaction_dt: date (nullable = true)\n",
      " |-- subscriber_sk: long (nullable = true)\n",
      " |-- subscriber_num: string (nullable = true)\n",
      " |-- ban_num: long (nullable = true)\n",
      " |-- price_plan_cd: string (nullable = true)\n",
      " |-- segment_cd: string (nullable = true)\n",
      " |-- subs_market_cd: string (nullable = true)\n",
      " |-- voice_min_1_H: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_2_H: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_1_R: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_2_R: decimal(38,6) (nullable = true)\n",
      " |-- gprs_mb_3_H: decimal(38,11) (nullable = true)\n",
      " |-- gprs_mb_3_R: decimal(38,11) (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read traffic data:\n",
    "\n",
    "table_name = 'beemetrics.DM_MOBILE_SUBSCRIBER_USAGE_DAILY'\n",
    "\n",
    "# start_date = datetime.date(2025, 6, 1) # -- !!!\n",
    "# end_date = datetime.date(2025, 9, 22) # -- !!!\n",
    "\n",
    "# start_date = datetime.date(2025, 9, 22) # -- !!!\n",
    "# end_date = datetime.date(2025, 10, 19) # -- !!!\n",
    "\n",
    "start_date = datetime.date(2025, 9, 29) # -- !!!\n",
    "end_date = datetime.date(2025, 10, 27) # -- !!!\n",
    "\n",
    "df = (\n",
    "   spark.read.table(table_name)\n",
    "    .where(F.col('transaction_dt') >= start_date)\n",
    "    .where(F.col('transaction_dt') <  end_date)\n",
    "    .where(F.col('business_type_cd') == 'B2B')\n",
    "    .where(~F.col('segment_cd').isin('GKA', 'GME', 'GLA', 'GSE', 'GSH'))\n",
    "    # .where(F.col('subs_market_cd') == 'VIP')      # --!!!!\n",
    "    # .where(F.col('account_type_cd').isin(['47', '120']))  # -- MK\n",
    "    .groupby(\n",
    "        F.col('transaction_dt'),\n",
    "        F.col('subscriber_sk'),\n",
    "        F.col('subscriber_num'),\n",
    "        F.col('ban_num'),\n",
    "        F.col('price_plan_cd'),\n",
    "        F.col('segment_cd'),\n",
    "        F.col('subs_market_cd'),\n",
    "        # F.col('call_direction_cd'),\n",
    "        # F.col('roaming_cd')\n",
    "    )\n",
    "    .agg(\n",
    "        F.sum(F.when( (F.col('activity_cd') == 'VOICE') & (F.col('call_direction_cd') == 1) & (F.col('roaming_cd') == 'H'), F.col('usage_amt')/60 )).alias('voice_min_1_H'),\n",
    "        F.sum(F.when( (F.col('activity_cd') == 'VOICE') & (F.col('call_direction_cd') == 2) & (F.col('roaming_cd') == 'H'), F.col('usage_amt')/60 )).alias('voice_min_2_H'),\n",
    "        F.sum(F.when( (F.col('activity_cd') == 'VOICE') & (F.col('call_direction_cd') == 1) & (F.col('roaming_cd') != 'H'), F.col('usage_amt')/60 )).alias('voice_min_1_R'),\n",
    "        F.sum(F.when( (F.col('activity_cd') == 'VOICE') & (F.col('call_direction_cd') == 2) & (F.col('roaming_cd') != 'H'), F.col('usage_amt')/60 )).alias('voice_min_2_R'),\n",
    "        #F.sum(expr(\"case when activity_cd = 'VOICE' then usage_amt end \"))\n",
    "        #F.sum(F.when(F.col('activity_cd').isin( ['VOICE_TO_MCC', 'VOICE_MAIL']), F.col('usage_amt')/60)).alias('voice_2_min'),\n",
    "        F.sum(F.when( (F.col('activity_cd') == 'GPRS') & (F.col('call_direction_cd') == 3) & (F.col('roaming_cd') == 'H'), F.col('usage_amt')/1024/1024)).alias('gprs_mb_3_H'),\n",
    "        F.sum(F.when( (F.col('activity_cd') == 'GPRS') & (F.col('call_direction_cd') == 3) & (F.col('roaming_cd') != 'H'), F.col('usage_amt')/1024/1024)).alias('gprs_mb_3_R')\n",
    "    )\n",
    "    # .join(df_target_sp_CORE, how = 'left', on = 'subscriber_sk')\n",
    ")\n",
    "\n",
    "print('num_rows:', df.count())  \n",
    "# print('num_cols:', len(df.columns))  \n",
    "df.printSchema()\n",
    "\n",
    "# num_rows: 374 362 711\n",
    "# num_rows: 70 742 349\n",
    "# num_rows: 91 127 187\n",
    "# 27.10.25: 93 498 382"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cca7a1-e7ac-4a2d-9c8d-c371b8c040fd",
   "metadata": {},
   "source": [
    "#### Count NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e36660a-358e-466b-a48e-ee56d11405e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subscriber_sk: long (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- voice_min_1_H_countNA: integer (nullable = false)\n",
      " |-- voice_min_2_H_countNA: integer (nullable = false)\n",
      " |-- voice_min_1_R_countNA: integer (nullable = false)\n",
      " |-- voice_min_2_R_countNA: integer (nullable = false)\n",
      " |-- gprs_mb_3_H_countNA: integer (nullable = false)\n",
      " |-- gprs_mb_3_R_countNA: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import weekofyear\n",
    "\n",
    "df_count_na = df.withColumn( 'voice_min_1_H_countNA', F.when(F.isnan(F.col('voice_min_1_H')), 1).otherwise(0) ) \\\n",
    "                .withColumn( 'voice_min_2_H_countNA', F.when(F.isnan(F.col('voice_min_2_H')), 1).otherwise(0) ) \\\n",
    "                .withColumn( 'voice_min_1_R_countNA', F.when(F.isnan(F.col('voice_min_1_R')), 1).otherwise(0) ) \\\n",
    "                .withColumn( 'voice_min_2_R_countNA', F.when(F.isnan(F.col('voice_min_2_R')), 1).otherwise(0) ) \\\n",
    "                .withColumn( 'gprs_mb_3_H_countNA', F.when(F.isnan(F.col('gprs_mb_3_H')), 1).otherwise(0) ) \\\n",
    "                .withColumn( 'gprs_mb_3_R_countNA', F.when(F.isnan(F.col('gprs_mb_3_R')), 1).otherwise(0) ) \\\n",
    "\n",
    "# df_count_na = df_count_na.withColumn('week_of_year', weekofyear(df_count_na.transaction_dt))\n",
    "df_count_na = df_count_na.withColumn('week_of_year', weekofyear(F.col('transaction_dt')))\n",
    "\n",
    "df_count_na = df_count_na.select('subscriber_sk', 'week_of_year', \n",
    "                                 'voice_min_1_H_countNA', 'voice_min_2_H_countNA',\n",
    "                                 'voice_min_1_R_countNA', 'voice_min_2_R_countNA',\n",
    "                                 'gprs_mb_3_H_countNA', 'gprs_mb_3_R_countNA'\n",
    "                                )\n",
    "\n",
    "df_count_na.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c5ff183-3c70-470c-ae07-adeaa839926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_na_agg = df_count_na.groupby('subscriber_sk', 'week_of_year') \\\n",
    "                   .agg(F.sum( F.coalesce(F.col('voice_min_1_H_countNA'), F.lit(0.0)) ).alias('voice_min_1_H_countNA'), \n",
    "                        F.sum( F.coalesce(F.col('voice_min_2_H_countNA'), F.lit(0.0)) ).alias('voice_min_2_H_countNA'), \n",
    "                        F.sum( F.coalesce(F.col('voice_min_1_R_countNA'), F.lit(0.0)) ).alias('voice_min_1_R_countNA'), \n",
    "                        F.sum( F.coalesce(F.col('voice_min_2_R_countNA'), F.lit(0.0)) ).alias('voice_min_2_R_countNA'), \n",
    "                        F.sum( F.coalesce(F.col('gprs_mb_3_H_countNA'), F.lit(0.0)) ).alias('gprs_mb_3_H_countNA'),\n",
    "                        F.sum( F.coalesce(F.col('gprs_mb_3_R_countNA'), F.lit(0.0)) ).alias('gprs_mb_3_R_countNA'),\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "868706a2-f4a7-4a39-93e1-22641ef221bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+---------------------+---------------------+-------------------+-------------------+\n",
      "|voice_min_1_H_countNA|voice_min_2_H_countNA|voice_min_1_R_countNA|voice_min_2_R_countNA|gprs_mb_3_H_countNA|gprs_mb_3_R_countNA|\n",
      "+---------------------+---------------------+---------------------+---------------------+-------------------+-------------------+\n",
      "|                  0.0|                  0.0|                  0.0|                  0.0|                0.0|                0.0|\n",
      "+---------------------+---------------------+---------------------+---------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count_na_agg.select('voice_min_1_H_countNA', 'voice_min_2_H_countNA', 'voice_min_1_R_countNA', 'voice_min_2_R_countNA', 'gprs_mb_3_H_countNA', 'gprs_mb_3_R_countNA').dropDuplicates().show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f6c137-4dbb-40a4-8827-e2fecdb0cfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_check = spark.read.table('b2b_stg.df_churn_all_pret_mnp_qual_repr_pt') \n",
    "# print('num_rows:', df_check.count())\n",
    "# df_check.show(3)\n",
    "\n",
    "# # num_rows: 381656880"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56b771-84f2-4976-a970-b7c8886dc3ee",
   "metadata": {},
   "source": [
    "#### 2. Обращаемость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e0bb54b-629d-4c8b-b05d-35f75cb95e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select {твои колонки}, n.reason_1, n.reason_2, n.reason_3, n.calls, n.dialog_time\n",
    "# from {твоя_таблица} a\n",
    "# left join b2b_stg.mns_churn_pretenzii_all n on a.subscriber_sk = n.subscriber_sk and a.{твоя дата} = n.dt\n",
    "\n",
    "# n.reason_1, n.reason_2, n.reason_3 – колонки с причинами обращения, при агрегации их можно просто сконкатить в одну строчку\n",
    "# n.calls – количество обращений в эту дату, при агрегации просто взять sum\n",
    "# n.dialog_time – время диалога, при агрегации просто взять sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fd526f8-baa3-4d2b-b67c-0be4ff9072f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ban_num: long (nullable = true)\n",
      " |-- subscriber_num: string (nullable = true)\n",
      " |-- subscriber_sk: long (nullable = true)\n",
      " |-- transaction_dt: date (nullable = true)\n",
      " |-- reason_1: string (nullable = true)\n",
      " |-- reason_2: string (nullable = true)\n",
      " |-- reason_3: string (nullable = true)\n",
      " |-- calls: float (nullable = true)\n",
      " |-- dialog_time: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "df_pret = (\n",
    "            spark.read.table('b2b_stg.mns_churn_pretenzii_all') \n",
    "            .withColumnRenamed('ban', 'ban_num')\n",
    "            .withColumnRenamed('ctn', 'subscriber_num')\n",
    "            .withColumn('ban_num', F.col('ban_num').cast(LongType()))\n",
    "            .withColumnRenamed('dt', 'transaction_dt')\n",
    "          )\n",
    "\n",
    " # |-- subscriber_sk: long (nullable = true)\n",
    " # |-- subscriber_num: string (nullable = true)\n",
    " # |-- ban_num: long (nullable = true)\n",
    "df_pret.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "493e354e-b203-49ed-b2ff-7579b570e470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 93498812\n",
      "root\n",
      " |-- subscriber_sk: long (nullable = true)\n",
      " |-- subscriber_num: string (nullable = true)\n",
      " |-- ban_num: long (nullable = true)\n",
      " |-- transaction_dt: date (nullable = true)\n",
      " |-- price_plan_cd: string (nullable = true)\n",
      " |-- segment_cd: string (nullable = true)\n",
      " |-- subs_market_cd: string (nullable = true)\n",
      " |-- voice_min_1_H: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_2_H: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_1_R: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_2_R: decimal(38,6) (nullable = true)\n",
      " |-- gprs_mb_3_H: decimal(38,11) (nullable = true)\n",
      " |-- gprs_mb_3_R: decimal(38,11) (nullable = true)\n",
      " |-- reason_1: string (nullable = true)\n",
      " |-- reason_2: string (nullable = true)\n",
      " |-- reason_3: string (nullable = true)\n",
      " |-- calls: float (nullable = true)\n",
      " |-- dialog_time: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.join(df_pret, how = 'left', on = ['subscriber_sk', 'subscriber_num', 'ban_num', 'transaction_dt'])\n",
    "# df = df.join(df_pret, how = 'left', on = ['subscriber_sk', 'ban_num', 'transaction_dt'])\n",
    "\n",
    "print('num_rows:', df.count()) \n",
    "df.printSchema()\n",
    "# num_rows: 400 418 864\n",
    "# num_rows: 400 420 566\n",
    "\n",
    "# num_rows: 70 742 349\n",
    "# num_rows: 70 742 620\n",
    "\n",
    "# num_rows: 91 127 187\n",
    "# num_rows: 91 127 525\n",
    "# 27.10.25: 93 498 697"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e83e4cd2-6fde-4f28-9108-d535701c5d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1702"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "400420566 - 400418864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74ae3eb8-9acc-484c-89f8-28011f9b5c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "93498697 - 93498382"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1dd0bb-9f3b-4aa2-a860-03a8083350e2",
   "metadata": {},
   "source": [
    "#### 4. Заявки MNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655886b9-fd65-4622-b0b7-96fe5e537eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subscriber_num: string (nullable = true)\n",
      " |-- transaction_dt: timestamp (nullable = true)\n",
      " |-- req_qnt: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -- MNP PortOut Requests:\n",
    "\n",
    "# start_date = datetime.date(2025, 6, 1) # -- !!!\n",
    "# end_date = datetime.date(2025, 9, 22) # -- !!!\n",
    "\n",
    "# start_date = datetime.date(2025, 9, 22) # -- !!!\n",
    "# end_date = datetime.date(2025, 10, 13) # -- !!!\n",
    "\n",
    "df_mnp = spark.sql(f\"\"\"select subs_key, req_created, count(distinct request_id) req_qnt\n",
    "                        from biis.dim_crm_mnp_requests_pub r\n",
    "                        where 1=1\n",
    "                        and process_type = 'PortOut'\n",
    "                        and req_created >= '{start_date}'\n",
    "                        and req_created < '{end_date}'\n",
    "                        group by subs_key, req_created\n",
    "                    \"\"\")\n",
    "\n",
    "df_mnp = df_mnp.withColumnRenamed('subs_key', 'subscriber_num') \\\n",
    "               .withColumnRenamed('req_created', 'transaction_dt') \n",
    "\n",
    "df_mnp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1246df88-f774-4106-935b-5411ae0598b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=====================================================>(397 + 3) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 93498812\n",
      "root\n",
      " |-- subscriber_num: string (nullable = true)\n",
      " |-- transaction_dt: date (nullable = true)\n",
      " |-- subscriber_sk: long (nullable = true)\n",
      " |-- ban_num: long (nullable = true)\n",
      " |-- price_plan_cd: string (nullable = true)\n",
      " |-- segment_cd: string (nullable = true)\n",
      " |-- subs_market_cd: string (nullable = true)\n",
      " |-- voice_min_1_H: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_2_H: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_1_R: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_2_R: decimal(38,6) (nullable = true)\n",
      " |-- gprs_mb_3_H: decimal(38,11) (nullable = true)\n",
      " |-- gprs_mb_3_R: decimal(38,11) (nullable = true)\n",
      " |-- reason_1: string (nullable = true)\n",
      " |-- reason_2: string (nullable = true)\n",
      " |-- reason_3: string (nullable = true)\n",
      " |-- calls: float (nullable = true)\n",
      " |-- dialog_time: float (nullable = true)\n",
      " |-- req_qnt: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.join(df_mnp, how = 'left', on = ['subscriber_num', 'transaction_dt'])\n",
    "\n",
    "print('num_rows:', df.count()) \n",
    "df.printSchema()\n",
    "\n",
    "# num_rows: 400420566\n",
    "# num_rows: 70 742 620\n",
    "# num_rows: 91 127 525\n",
    "# 27.10.25: 93 498 697"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44dc2a6c-62f4-4995-b5eb-3d5506aaf3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select('req_qnt').dropDuplicates().show()\n",
    "# df.select('req_qnt').toPandas().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b293d0-0668-4397-9f59-e50429e7661e",
   "metadata": {},
   "source": [
    "#### 5. Качество сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1ef88fb-98f4-42d8-b248-f4b62abc2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Качество связи (тут ban = float):\n",
    "# select {твои колонки}, m.active_days_data, m.bad_days_data, m.accept_days_data, m.good_days_data\n",
    "# from {твоя_таблица} a\n",
    "# left b2b_stg.mns_quality_internet_weekly_without_agg n on a.subscriber_sk = n.subscriber_sk and a.{твоя дата} = TO_TIMESTAMP(m.event_date, 'YYYY-MM-DD HH24:MI:SS') and a.{тсвой бан} = n.ban and a.{номер телефона} =  n.msisdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8229e784-2d44-4111-a3ea-e04ff9ad61ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subscriber_sk: long (nullable = true)\n",
      " |-- subscriber_num: string (nullable = true)\n",
      " |-- ban_num: long (nullable = true)\n",
      " |-- client_inn: string (nullable = true)\n",
      " |-- transaction_dt: date (nullable = true)\n",
      " |-- active_days_data: integer (nullable = true)\n",
      " |-- bad_days_data: integer (nullable = true)\n",
      " |-- accept_days_data: integer (nullable = true)\n",
      " |-- good_days_data: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "df_qual = (\n",
    "            spark.read.table('b2b_stg.mns_quality_internet_weekly_without_agg') \n",
    "            .withColumnRenamed('ban', 'ban_num')\n",
    "            .withColumnRenamed('msisdn', 'subscriber_num')\n",
    "            .withColumn('ban_num', F.col('ban_num').cast(LongType()))\n",
    "            .withColumnRenamed('event_date', 'transaction_dt')\n",
    "          )\n",
    "\n",
    " # |-- subscriber_sk: long (nullable = true)\n",
    " # |-- subscriber_num: string (nullable = true)\n",
    " # |-- ban_num: long (nullable = true)\n",
    "df_qual.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48f37971-4c64-40f8-8f6d-811b59fdac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=====================================================>(399 + 1) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 94506341\n",
      "root\n",
      " |-- subscriber_sk: long (nullable = true)\n",
      " |-- subscriber_num: string (nullable = true)\n",
      " |-- ban_num: long (nullable = true)\n",
      " |-- transaction_dt: date (nullable = true)\n",
      " |-- price_plan_cd: string (nullable = true)\n",
      " |-- segment_cd: string (nullable = true)\n",
      " |-- subs_market_cd: string (nullable = true)\n",
      " |-- voice_min_1_H: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_2_H: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_1_R: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_2_R: decimal(38,6) (nullable = true)\n",
      " |-- gprs_mb_3_H: decimal(38,11) (nullable = true)\n",
      " |-- gprs_mb_3_R: decimal(38,11) (nullable = true)\n",
      " |-- reason_1: string (nullable = true)\n",
      " |-- reason_2: string (nullable = true)\n",
      " |-- reason_3: string (nullable = true)\n",
      " |-- calls: float (nullable = true)\n",
      " |-- dialog_time: float (nullable = true)\n",
      " |-- req_qnt: long (nullable = true)\n",
      " |-- client_inn: string (nullable = true)\n",
      " |-- active_days_data: integer (nullable = true)\n",
      " |-- bad_days_data: integer (nullable = true)\n",
      " |-- accept_days_data: integer (nullable = true)\n",
      " |-- good_days_data: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.join(df_qual, how = 'left', on = ['subscriber_sk', 'subscriber_num', 'ban_num', 'transaction_dt'])\n",
    "\n",
    "print('num_rows:', df.count()) \n",
    "df.printSchema()\n",
    "\n",
    "# before: num_rows: 70 742 620\n",
    "# after: num_rows: 71 451 133\n",
    "\n",
    "# before: num_rows: 91 127 525\n",
    "# after: num_rows: 92 023 384\n",
    "# 27.10.25: 93 498 697"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5a04960-ccd1-4496-85d2-ca49dbefcfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708513"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "71451133 - 70742620"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68232dde-3617-495a-af73-1ece38be4b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1007644"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "94506341 - 93498697"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34425b63-2a40-42a5-a9c3-adc356b263a1",
   "metadata": {},
   "source": [
    "#### 7. Попадание в Reprice 8. Изменение TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bb9917b-67ff-4b31-96fe-41eb9191987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Сбор 7 и 8:\n",
    "\n",
    "# res_churn_table = spark.sql('''\n",
    "#                     select k.*, b.price_plan_type, case when m.repr2q2025b = 1 or m.repr2q2025s = 1 then 1 else 0 end as repr2q2025\n",
    "#                     from b2b_stg.df_churn_all_pret_mnp_qual k\n",
    "#                     left join \n",
    "#                     (select distinct price_plan_cd, effective_dt, price_plan_type\n",
    "#                     from b2bba_prod.price_plan_pub) b on k.price_plan_cd = b.price_plan_cd and k.transaction_dt >= b.effective_dt\n",
    "#                     left join \n",
    "#                     (select distinct subs_key, ban_key, subscriber_id, repr2q2025b, repr2q2025s\n",
    "#                     from b2b_cltv_stg.map_of_base_20250620) m on k.subscriber_num = m.subs_key and k.ban_num = CAST(m.ban_key AS STRING) and k.subscriber_sk = m.subscriber_id\n",
    "#                     ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "968f152d-d6a0-4a70-a551-fe63449c0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resulting_table = 'b2b_stg.df_churn_all_pret_mnp_qual_repr_pt'\n",
    "\n",
    "# db_name, table_name = resulting_table.split(\".\")\n",
    "   \n",
    "# (\n",
    "#     res_churn_table\n",
    "#     .repartition(40)\n",
    "#     .write.mode(\"overwrite\")\n",
    "#     .format(\"parquet\")\n",
    "#     .option(\"path\", f\"hdfs://ns-etl/warehouse/tablespace/external/hive/{db_name}.db/{table_name}\")\n",
    "#     .saveAsTable(resulting_table)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35475143-99d1-475a-8938-2b8127670500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.table('b2b_stg.df_churn_all_pret_mnp_qual_repr_pt') \n",
    "        \n",
    "# print(df.count())\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9121b4b-9935-41fc-b6a0-fae4e7fecca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для 7, где k – основная таблица:\n",
    "# res_churn_table = spark.sql('''select k.*, m.repr2q2025\n",
    "#     from (основная таблица) k\n",
    "#     left join b2b_stg.mns_churn_reprice m \n",
    "#     on k.subscriber_num = m.subs_key and k.ban_num = CAST(m.ban_key AS STRING) and k.subscriber_sk = m.subscriber_id\n",
    "# ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95f6b86d-8eb8-41aa-be58-3798584ce32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:======================================================(400 + 0) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 94506341\n",
      "root\n",
      " |-- subscriber_sk: long (nullable = true)\n",
      " |-- subscriber_num: string (nullable = true)\n",
      " |-- ban_num: long (nullable = true)\n",
      " |-- transaction_dt: date (nullable = true)\n",
      " |-- price_plan_cd: string (nullable = true)\n",
      " |-- segment_cd: string (nullable = true)\n",
      " |-- subs_market_cd: string (nullable = true)\n",
      " |-- voice_min_1_H: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_2_H: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_1_R: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_2_R: decimal(38,6) (nullable = true)\n",
      " |-- gprs_mb_3_H: decimal(38,11) (nullable = true)\n",
      " |-- gprs_mb_3_R: decimal(38,11) (nullable = true)\n",
      " |-- reason_1: string (nullable = true)\n",
      " |-- reason_2: string (nullable = true)\n",
      " |-- reason_3: string (nullable = true)\n",
      " |-- calls: float (nullable = true)\n",
      " |-- dialog_time: float (nullable = true)\n",
      " |-- req_qnt: long (nullable = true)\n",
      " |-- client_inn: string (nullable = true)\n",
      " |-- active_days_data: integer (nullable = true)\n",
      " |-- bad_days_data: integer (nullable = true)\n",
      " |-- accept_days_data: integer (nullable = true)\n",
      " |-- good_days_data: integer (nullable = true)\n",
      " |-- ban_key: long (nullable = true)\n",
      " |-- repr2q2025: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_repr = ( \n",
    "            spark.read.table('b2b_stg.mns_churn_reprice') \n",
    "            .withColumnRenamed('subs_key', 'subscriber_num')\n",
    "            .withColumn('ban_num', F.col('ban_key').cast(StringType()))\n",
    "            .withColumnRenamed('subscriber_id', 'subscriber_sk')\n",
    "          )\n",
    "\n",
    "df = df.join(df_repr, how = 'left', on = ['subscriber_sk', 'subscriber_num', 'ban_num'])\n",
    "\n",
    "print('num_rows:', df.count()) \n",
    "df.printSchema()\n",
    "\n",
    "# num_rows: 71451133\n",
    "# num_rows: 92 023 384\n",
    "# num_rows: 94 506 341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "193a1281-7b7b-4bee-8cd6-cf3121d1365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Для 8, где k – основная таблица:\n",
    "# select k.*, b.price_plan_type\n",
    "# from (основная таблица) k\n",
    "# left join b2b_stg.mns_churn_price_plan b\n",
    "# on k.price_plan_cd = b.price_plan_cd and k.transaction_dt >= b.effective_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3cb8116-f263-4032-9ffa-bd2b7b327b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:=====================================================>(399 + 1) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 94506341\n",
      "root\n",
      " |-- subscriber_sk: long (nullable = true)\n",
      " |-- subscriber_num: string (nullable = true)\n",
      " |-- ban_num: long (nullable = true)\n",
      " |-- transaction_dt: date (nullable = true)\n",
      " |-- price_plan_cd: string (nullable = true)\n",
      " |-- segment_cd: string (nullable = true)\n",
      " |-- subs_market_cd: string (nullable = true)\n",
      " |-- voice_min_1_H: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_2_H: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_1_R: decimal(38,6) (nullable = true)\n",
      " |-- voice_min_2_R: decimal(38,6) (nullable = true)\n",
      " |-- gprs_mb_3_H: decimal(38,11) (nullable = true)\n",
      " |-- gprs_mb_3_R: decimal(38,11) (nullable = true)\n",
      " |-- reason_1: string (nullable = true)\n",
      " |-- reason_2: string (nullable = true)\n",
      " |-- reason_3: string (nullable = true)\n",
      " |-- calls: float (nullable = true)\n",
      " |-- dialog_time: float (nullable = true)\n",
      " |-- req_qnt: long (nullable = true)\n",
      " |-- client_inn: string (nullable = true)\n",
      " |-- active_days_data: integer (nullable = true)\n",
      " |-- bad_days_data: integer (nullable = true)\n",
      " |-- accept_days_data: integer (nullable = true)\n",
      " |-- good_days_data: integer (nullable = true)\n",
      " |-- ban_key: long (nullable = true)\n",
      " |-- repr2q2025: integer (nullable = true)\n",
      " |-- price_plan_type: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_change_tp = spark.read.table('b2b_stg.mns_churn_price_plan') \n",
    "\n",
    "df =  df.alias(\"k\").join(df_change_tp.alias(\"b\"),\n",
    "                        (F.col(\"k.price_plan_cd\") == F.col(\"b.price_plan_cd\")) &\n",
    "                        (F.col(\"k.transaction_dt\") >= F.col(\"b.effective_dt\")),\n",
    "                        how = \"left\") \\\n",
    "                    .select(\"k.*\", \"b.price_plan_type\")\n",
    "\n",
    "print('num_rows:', df.count()) \n",
    "df.printSchema()\n",
    "\n",
    "# num_rows: 71451133\n",
    "# num_rows: 92023384\n",
    "# num_rows: 94506341"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1cbac-f917-4c30-917f-9181a1b95f97",
   "metadata": {},
   "source": [
    "#### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "009f8c55-29b4-4a64-9b97-1aa484c944db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import weekofyear\n",
    "\n",
    "df = df.withColumn('week_of_year', weekofyear(F.col('transaction_dt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a2a9cf5-2500-4da1-926b-268b4e55062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 94:================================================>    (365 + 24) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 19873521\n",
      "root\n",
      " |-- subscriber_sk: long (nullable = true)\n",
      " |-- subs_market_cd: string (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- voice_min_1_H_avg: double (nullable = true)\n",
      " |-- voice_min_2_H_avg: double (nullable = true)\n",
      " |-- voice_min_1_R_avg: double (nullable = true)\n",
      " |-- voice_min_2_R_avg: double (nullable = true)\n",
      " |-- gprs_mb_3_H_avg: double (nullable = true)\n",
      " |-- gprs_mb_3_R_avg: double (nullable = true)\n",
      " |-- reason_1: string (nullable = true)\n",
      " |-- reason_2: string (nullable = true)\n",
      " |-- reason_3: string (nullable = true)\n",
      " |-- calls_sum: double (nullable = true)\n",
      " |-- dialog_time_sum: double (nullable = true)\n",
      " |-- req_qnt_sum: double (nullable = true)\n",
      " |-- active_days_sum: double (nullable = true)\n",
      " |-- bad_days_sum: double (nullable = true)\n",
      " |-- accept_days_sum: double (nullable = true)\n",
      " |-- good_days_sum: double (nullable = true)\n",
      " |-- price_plan_type_max: string (nullable = true)\n",
      " |-- repr2q2025_max: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df_agg = df.groupby('subscriber_sk', 'segment_cd', 'subs_market_cd', 'week_of_year') \\\n",
    "df_agg = df.groupby('subscriber_sk', 'subs_market_cd', 'week_of_year') \\\n",
    "           .agg(F.mean( F.coalesce(F.col('voice_min_1_H'), F.lit(0.0)) ).alias('voice_min_1_H_avg'), \n",
    "                F.mean( F.coalesce(F.col('voice_min_2_H'), F.lit(0.0)) ).alias('voice_min_2_H_avg'), \n",
    "                F.mean( F.coalesce(F.col('voice_min_1_R'), F.lit(0.0)) ).alias('voice_min_1_R_avg'), \n",
    "                F.mean( F.coalesce(F.col('voice_min_2_R'), F.lit(0.0)) ).alias('voice_min_2_R_avg'), \n",
    "                F.mean( F.coalesce(F.col('gprs_mb_3_H'), F.lit(0.0)) ).alias('gprs_mb_3_H_avg'),\n",
    "                F.mean( F.coalesce(F.col('gprs_mb_3_R'), F.lit(0.0)) ).alias('gprs_mb_3_R_avg'),\n",
    "\n",
    "                # F.concat(F.col('reason_1'), F.col('reason_2'), F.col('reason_3')).alias('pretenzii_reason'),\n",
    "                F.max('reason_1').alias('reason_1'),\n",
    "                F.max('reason_2').alias('reason_2'),\n",
    "                F.max('reason_3').alias('reason_3'),\n",
    "                \n",
    "                F.sum( F.coalesce(F.col('calls'), F.lit(0.0)) ).alias('calls_sum'),\n",
    "                F.sum( F.coalesce(F.col('dialog_time'), F.lit(0.0)) ).alias('dialog_time_sum'),\n",
    "                F.sum( F.coalesce(F.col('req_qnt'), F.lit(0.0)) ).alias('req_qnt_sum'),\n",
    "                      \n",
    "                F.sum( F.coalesce(F.col('active_days_data'), F.lit(0.0)) ).alias('active_days_sum'),\n",
    "                F.sum( F.coalesce(F.col('bad_days_data'), F.lit(0.0)) ).alias('bad_days_sum'),\n",
    "                F.sum( F.coalesce(F.col('accept_days_data'), F.lit(0.0)) ).alias('accept_days_sum'),\n",
    "                F.sum( F.coalesce(F.col('good_days_data'), F.lit(0.0)) ).alias('good_days_sum'),\n",
    "                      \n",
    "                F.max('price_plan_type').alias('price_plan_type_max'),\n",
    "                F.max('repr2q2025').alias('repr2q2025_max')\n",
    "             )\n",
    "\n",
    "print('num_rows:', df_agg.count())\n",
    "df_agg.printSchema()\n",
    "\n",
    "# num_rows: 71 451 133\n",
    "# num_rows: 15 225 495\n",
    "# num_rows: 20 081 473\n",
    "# num_rows: 19 873 521"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c9623-4bac-437a-9b38-0d75ae7a07d1",
   "metadata": {},
   "source": [
    "#### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eee89055-9c48-49ac-8c30-6c35264e3cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_date: 2025-09-29\n",
      "end_date: 2025-10-27\n"
     ]
    }
   ],
   "source": [
    "print('start_date:', start_date)\n",
    "print('end_date:', end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35744074-dc3e-44e2-acfa-5dd6cee7d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import weekofyear\n",
    "\n",
    "# start_date = datetime.date(2025, 6, 1) \n",
    "# end_date = datetime.date(2025, 9, 22)\n",
    "\n",
    "# start_date = datetime.date(2025, 9, 22) # -- !!!\n",
    "# end_date = datetime.date(2025, 10, 20) # -- !!!\n",
    "\n",
    "df_churn_target = ( spark.read.table(\"beemetrics.dm_mobile_subscriber\") \n",
    "                  .where(F.col(\"calendar_dt\") >= start_date)\n",
    "                  .where(F.col(\"calendar_dt\") < end_date)\n",
    "                  .where(F.col(\"business_type_cd\") == \"B2B\") \n",
    "                  .where(F.col(\"churn_flg\") == 1) \n",
    "                  .select( F.col('subscriber_sk'), \n",
    "                           F.col('calendar_dt').alias('churn_dt'), \n",
    "                           F.col('churn_flg')\n",
    "                         ) )\n",
    "\n",
    "df_churn_target = df_churn_target.withColumn('week_of_year', weekofyear(df_churn_target.churn_dt))\n",
    "\n",
    "df_churn_target_agg = df_churn_target.groupby('subscriber_sk', 'week_of_year') \\\n",
    "                                     .agg(F.max('churn_flg').alias('churn_flg'))\n",
    "\n",
    "# print('num_rows:', df_churn_target_agg.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a370327a-ba93-475d-bc64-4983b32f1265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:==================================================> (390 + 10) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 19873521\n",
      "root\n",
      " |-- subscriber_sk: long (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- subs_market_cd: string (nullable = true)\n",
      " |-- voice_min_1_H_avg: double (nullable = true)\n",
      " |-- voice_min_2_H_avg: double (nullable = true)\n",
      " |-- voice_min_1_R_avg: double (nullable = true)\n",
      " |-- voice_min_2_R_avg: double (nullable = true)\n",
      " |-- gprs_mb_3_H_avg: double (nullable = true)\n",
      " |-- gprs_mb_3_R_avg: double (nullable = true)\n",
      " |-- reason_1: string (nullable = true)\n",
      " |-- reason_2: string (nullable = true)\n",
      " |-- reason_3: string (nullable = true)\n",
      " |-- calls_sum: double (nullable = true)\n",
      " |-- dialog_time_sum: double (nullable = true)\n",
      " |-- req_qnt_sum: double (nullable = true)\n",
      " |-- active_days_sum: double (nullable = true)\n",
      " |-- bad_days_sum: double (nullable = true)\n",
      " |-- accept_days_sum: double (nullable = true)\n",
      " |-- good_days_sum: double (nullable = true)\n",
      " |-- price_plan_type_max: string (nullable = true)\n",
      " |-- repr2q2025_max: integer (nullable = true)\n",
      " |-- churn_flg: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_agg = df_agg.join(df_churn_target_agg, how = 'left', on = ['subscriber_sk', 'week_of_year'])\n",
    "\n",
    "print('num_rows:', df_agg.count())\n",
    "df_agg.printSchema()\n",
    "\n",
    "# num_rows: 80 000 219\n",
    "# num_rows: 15 225 495\n",
    "# num_rows: 20 081 473\n",
    "# num_rows: 19 873 521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cbb3330-c2ba-4086-83b0-fd0d8d946c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_agg.sort(['subscriber_sk', 'week_of_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98f776f9-8761-4953-bf3b-83a06dc67e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:====================================================>(397 + 3) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 19873521\n",
      "root\n",
      " |-- subscriber_sk: long (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- subs_market_cd: string (nullable = true)\n",
      " |-- voice_min_1_H_avg: double (nullable = true)\n",
      " |-- voice_min_2_H_avg: double (nullable = true)\n",
      " |-- voice_min_1_R_avg: double (nullable = true)\n",
      " |-- voice_min_2_R_avg: double (nullable = true)\n",
      " |-- gprs_mb_3_H_avg: double (nullable = true)\n",
      " |-- gprs_mb_3_R_avg: double (nullable = true)\n",
      " |-- reason_1: string (nullable = true)\n",
      " |-- reason_2: string (nullable = true)\n",
      " |-- reason_3: string (nullable = true)\n",
      " |-- calls_sum: double (nullable = true)\n",
      " |-- dialog_time_sum: double (nullable = true)\n",
      " |-- req_qnt_sum: double (nullable = true)\n",
      " |-- active_days_sum: double (nullable = true)\n",
      " |-- bad_days_sum: double (nullable = true)\n",
      " |-- accept_days_sum: double (nullable = true)\n",
      " |-- good_days_sum: double (nullable = true)\n",
      " |-- price_plan_type_max: string (nullable = true)\n",
      " |-- repr2q2025_max: integer (nullable = true)\n",
      " |-- churn_flg: integer (nullable = true)\n",
      " |-- voice_min_1_H_countNA: double (nullable = true)\n",
      " |-- voice_min_2_H_countNA: double (nullable = true)\n",
      " |-- voice_min_1_R_countNA: double (nullable = true)\n",
      " |-- voice_min_2_R_countNA: double (nullable = true)\n",
      " |-- gprs_mb_3_H_countNA: double (nullable = true)\n",
      " |-- gprs_mb_3_R_countNA: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_agg = df_agg.join(df_count_na_agg, how = 'left', on = ['subscriber_sk', 'week_of_year'])\n",
    "\n",
    "print('num_rows:', df_agg.count())\n",
    "df_agg.printSchema()\n",
    "\n",
    "# num_rows: 15 225 495\n",
    "# num_rows: 20 081 473\n",
    "# num_rows: 19 873 521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "029bfd0d-58c7-4ce2-8d4e-e02da66ceaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                0]]\r"
     ]
    }
   ],
   "source": [
    "# resulting_table = 'b2b_stg.df_churn_all_agg_2'\n",
    "# resulting_table = 'b2b_stg.df_churn_all_agg_131025'\n",
    "# resulting_table = 'b2b_stg.df_churn_all_agg_201025'\n",
    "resulting_table = 'b2b_stg.df_churn_all_agg_271025'\n",
    "\n",
    "db_name, table_name = resulting_table.split(\".\")\n",
    "   \n",
    "(\n",
    "    df_agg\n",
    "    .repartition(40)\n",
    "    .write.mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", f\"hdfs://ns-etl/warehouse/tablespace/external/hive/{db_name}.db/{table_name}\")\n",
    "    .saveAsTable(resulting_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f38ceb7-85dd-4710-bb5c-fe7433c5de66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:==============================================>      (105 + 0) / 120]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 19873521\n",
      "root\n",
      " |-- subscriber_sk: long (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- subs_market_cd: string (nullable = true)\n",
      " |-- voice_min_1_H_avg: double (nullable = true)\n",
      " |-- voice_min_2_H_avg: double (nullable = true)\n",
      " |-- voice_min_1_R_avg: double (nullable = true)\n",
      " |-- voice_min_2_R_avg: double (nullable = true)\n",
      " |-- gprs_mb_3_H_avg: double (nullable = true)\n",
      " |-- gprs_mb_3_R_avg: double (nullable = true)\n",
      " |-- reason_1: string (nullable = true)\n",
      " |-- reason_2: string (nullable = true)\n",
      " |-- reason_3: string (nullable = true)\n",
      " |-- calls_sum: double (nullable = true)\n",
      " |-- dialog_time_sum: double (nullable = true)\n",
      " |-- req_qnt_sum: double (nullable = true)\n",
      " |-- active_days_sum: double (nullable = true)\n",
      " |-- bad_days_sum: double (nullable = true)\n",
      " |-- accept_days_sum: double (nullable = true)\n",
      " |-- good_days_sum: double (nullable = true)\n",
      " |-- price_plan_type_max: string (nullable = true)\n",
      " |-- repr2q2025_max: integer (nullable = true)\n",
      " |-- churn_flg: integer (nullable = true)\n",
      " |-- voice_min_1_H_countNA: double (nullable = true)\n",
      " |-- voice_min_2_H_countNA: double (nullable = true)\n",
      " |-- voice_min_1_R_countNA: double (nullable = true)\n",
      " |-- voice_min_2_R_countNA: double (nullable = true)\n",
      " |-- gprs_mb_3_H_countNA: double (nullable = true)\n",
      " |-- gprs_mb_3_R_countNA: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_check = spark.read.table(resulting_table)\n",
    "\n",
    "print('num_rows:', df_check.count())\n",
    "df_check.printSchema()\n",
    "\n",
    "# num_rows: 15 225 495\n",
    "# num_rows: 20 081 473\n",
    "# num_rows: 19 873 521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3b127-4440-445b-babe-ebf770736705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51206a30-4ab1-4471-98ee-6e5a28e37f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 3.5.1",
   "language": "python",
   "name": "pyspark-3.5.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
